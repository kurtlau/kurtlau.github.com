---
layout: post
category : Linux
title: loadavg——从用户态到内核态的源码分析
---

公司的同事们经常会用到loadavg分析系统的负载情况，但是又都无法准确描述loadavg的具体含义是什么。

![uptime](/img/2013-04-14-uptime.jpg)

如上图`uptime`命令的最后三个数字0.29、0.06、0.02，分别表示的是系统在最近1分钟、5分钟、15分钟的系统负载。
但系统负载到底是怎么定义的呢？本文将从用户态命令到procfs到内核态源码一步一步的找到上文显示的三个系统负载值是如何算出来的。

**1. procps**

procps是托管在sourceforge上的一个开源项目，主页在[procps.sourceforge.net](http://procps.sourceforge.net)。
我们系统中常见的观察系统性能、进程状态、进程操作的很多命令都由此项目提供，如：

- `free`查看系统内存使用情况
- `pgrep`查找对应进程
- 以及更加常用`ps`、`top`和`vmstat`等命令。

procps-3.2.8源代码的目录结构非常简单，根目录下是一堆命令的源代码和man文件，proc和ps目录下是一些读取/proc文件系统的公共函数。
获取系统负载的loadavg函数的定义在sysinfo.c中。
代码的功能十分简单，就是从/proc/loadavg文件中读取三个浮点数，而这三个浮点数就对应了系统在最近1分钟、5分钟、15分钟的系统负载。
继续来看proc文件系统中的这三个值是如何计算出来的。

**2. proc文件系统**

proc文件系统的简介见[维基百科](http://en.wikipedia.org/wiki/Procfs#Linux)。procps主要读取proc文件系统的数据进行展示。

![loadavg_proc](/img/2013-04-14-loadavg_proc.png)

以Linux-2.6.34.13版本内核为例，读取/proc/loadavg的在内核中的调用关系大概如上图。内核在proc文件系统中注册的fop会调用loadavg\_proc\_show函数，
最终调用到[get\_avenrun](http://lxr.oss.org.cn/source/kernel/sched.c?v=2.6.34#L3036)函数。
而get\_avenrun函数直接读取了avenrun数组。需要说明的是这个offset字段是后面计算小数时的四舍五入。

接下来继续看内核是如何将计算好的系统负载填入avenrun数组的。系统在每个时钟中断都会执行calc\_global\_load函数，
该函数通过读取atomic64\_t类型的全局变量calc\_load\_tasks经过一系列的计算，将计算结果放到avenrun数组中，计算过程稍后在讨论，
我们先看一下系统是什么时候更新calc\_load\_tasks的。

还好用到calc\_load\_tasks变量的地方[不多](http://lxr.oss.org.cn/ident?v=2.6.34;i=calc_load_tasks)，
内核中只有三个个函数用到了calc\_load\_tasks变量。
其中只有calc\_load\_account\_active函数对calc\_load\_tasks执行了atomic\_long\_add操作，而calc\_global\_load\_remove函数则是calc\_load\_tasks。
先看calc\_load\_account\_active函数。

<pre class="prettyprint lang-c linenums">
static void calc_load_account_active(struct rq *this_rq)
{
	long nr_active, delta;

	nr_active = this_rq->nr_running;
	nr_active += (long) this_rq->nr_uninterruptible;

	if (nr_active != this_rq->calc_load_active) {
		delta = nr_active - this_rq->calc_load_active;
		this_rq->calc_load_active = nr_active;
		atomic_long_add(delta, &calc_load_tasks);
	}
}
</pre>

上面代码中5、6行读取了当前struct rq中处于running状态和uninterruptible状态的线程数并计算与上次统计的差值加到calc\_load_\tasks上。
我们知道struct rq是Linux进程调度的基本数据结构，在每个CPU都对应了一个struct rq，
而每个可运行的线程都唯一归属于一个struct rq。
因此，可知：

>calc\_load\_tasks结构中保存的是当前系统中处于running状态和uninterruptible状态的线程总数

**3. 活跃线程数到系统负载的计算**

内核函数[calc\_load](http://lxr.oss.org.cn/source/kernel/sched.c?v=2.6.34#L3044)通过上次系统负载值和当前活跃线程数计算此时的系统负载。
公式如下：

![负载计算公式](/img/2013-04-14-ema.png)

先以第一个公式为例，解释一下公式中一目了然的部分。

* ![LoadAvgn](/img/2013-04-14-loadavgn.png)对应的是系统最近这一分钟的负载
* ![LoadAvgn-1](/img/2013-04-14-loadavgn-1.png)对应的是5秒钟前计算出的系统最近一分钟的负载
* 5/60是因为系统5秒钟检测一次负载
* a(n)对应的是系统当前活跃线程数

接下来是公式中的重点部分，这是个神马公式？？

目前大部分的中文文章中对于这部分使用“假设进程的生存时间符合指数分布”来解释。
其实Linux内核内核使用的是称之为
[指数平滑法（Exponential\_smoothing）](http://en.wikipedia.org/wiki/Exponential_smoothing)或
[指数平均移动（Exponential Moving Average）](http://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average)
的方法，通过每5秒获得一个当前活跃线程数的一个一个离散的点绘制出一个相对平滑而又符合系统负载趋势的曲线。如下图

![plot](/img/2013-04-14-plot.png)